# Baseline ViT Configuration for CIFAR-10

model:
  name: vit_small
  img_size: 32
  patch_size: 4
  in_channels: 3
  num_classes: 10
  embed_dim: 256
  depth: 6
  num_heads: 8
  mlp_ratio: 4.0
  dropout: 0.1

training:
  batch_size: 128
  epochs: 50
  learning_rate: 0.001
  weight_decay: 0.01
  warmup_epochs: 5

  # Gradient clipping
  gradient_clip: 1.0

  # Mixed precision training
  use_amp: true

optimizer:
  name: adamw
  betas: [0.9, 0.999]
  eps: 1.0e-8

scheduler:
  name: cosine
  eta_min: 1.0e-6

data:
  dataset: cifar10
  augmentation: true
  num_workers: 4

system:
  seed: 42
  precision: 16
  deterministic: true

logging:
  use_wandb: true
  log_every_n_steps: 10
  save_top_k: 3
